{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c267e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch_pruning as tp\n",
    "\n",
    "import pbench\n",
    "pbench.forward_patch.patch_timm_forward() # patch timm.forward() to support pruning\n",
    "\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import torchvision as tv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e85d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "def prepare_imagenet(imagenet_root, train_batch_size=64, val_batch_size=128, num_workers=4, use_imagenet_mean_std=True, interpolation='bicubic', val_resize=256):\n",
    "    \"\"\"The imagenet_root should contain train and val folders.\n",
    "    \"\"\"\n",
    "    interpolation = getattr(T.InterpolationMode, interpolation.upper())\n",
    "\n",
    "    print('Parsing dataset...')\n",
    "    train_dst = ImageFolder(os.path.join(imagenet_root, 'train'), \n",
    "                            transform=pbench.data.presets.ClassificationPresetEval(\n",
    "                                mean=[0.485, 0.456, 0.406] if use_imagenet_mean_std else [0.5, 0.5, 0.5],\n",
    "                                std=[0.229, 0.224, 0.225] if use_imagenet_mean_std else [0.5, 0.5, 0.5],\n",
    "                                crop_size=224,\n",
    "                                resize_size=val_resize,\n",
    "                                interpolation=interpolation,\n",
    "                            )\n",
    "    )\n",
    "    val_dst = ImageFolder(os.path.join(imagenet_root, 'val'), \n",
    "                          transform=pbench.data.presets.ClassificationPresetEval(\n",
    "                                mean=[0.485, 0.456, 0.406] if use_imagenet_mean_std else [0.5, 0.5, 0.5],\n",
    "                                std=[0.229, 0.224, 0.225] if use_imagenet_mean_std else [0.5, 0.5, 0.5],\n",
    "                                crop_size=224,\n",
    "                                resize_size=val_resize,\n",
    "                                interpolation=interpolation,\n",
    "                            )\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dst, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dst, batch_size=val_batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    cal_subset_size = 2048\n",
    "    cal_indices = random.sample(range(len(val_loader)*val_batch_size), cal_subset_size)\n",
    "    cal_dst = Subset(val_dst, cal_indices)\n",
    "    cal_loader = torch.utils.data.DataLoader(cal_dst, batch_size=val_batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, val_loader, cal_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17d297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for k, (images, labels) in enumerate(tqdm(val_loader)):  \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss += torch.nn.functional.cross_entropy(outputs, labels, reduction='sum').item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    del images, outputs, predicted\n",
    "    gc.collect()\n",
    "    return correct / len(val_loader.dataset), loss / len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b0efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path ='data/imagenet'\n",
    "train_batch_size = 64\n",
    "val_batch_size = 4\n",
    "no_imagenet_mean_std = False\n",
    "val_resize = 256\n",
    "interpolation = 'bilinear' #'bicubic' \n",
    "\n",
    "model = 'mobilenet_v2'  #'resnet101.tv_in1k' #'convnext_base.fb_in1k' \n",
    "is_torchvision = True\n",
    "drop = 0.0\n",
    "drop_path = 0.0\n",
    "ckpt = None\n",
    "\n",
    "taylor_batchs = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c6485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing dataset...\n",
      "Loading torchvision model mobilenet_v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ict317-3/Mohammad/Isomorphic-Pruning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ict317-3/Mohammad/Isomorphic-Pruning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "example_inputs = torch.randn(1,3,224,224)\n",
    "train_loader, val_loader, cal_loader = prepare_imagenet(data_path, train_batch_size=train_batch_size, val_batch_size=val_batch_size, use_imagenet_mean_std= not no_imagenet_mean_std, val_resize=val_resize, interpolation=interpolation)\n",
    "\n",
    "if is_torchvision:\n",
    "        import torchvision\n",
    "        print(f\"Loading torchvision model {model}...\")\n",
    "        model = torchvision.models.__dict__[model](pretrained=True).eval()\n",
    "else:\n",
    "    print(f\"Loading timm model {model}...\")\n",
    "    model = timm.create_model(model, pretrained=True, drop_rate=drop, drop_path_rate=drop_path).eval()\n",
    "\n",
    "if ckpt is not None:\n",
    "    print(f\"Loading checkpoint from {ckpt}...\")\n",
    "    ckpt = torch.load(ckpt, map_location='cpu')\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab19ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_blocks(model):\n",
    "\n",
    "    model_blocks, ignored_blocks = [], []\n",
    "\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            model_blocks.extend([sub_child for sub_child in child.children()])\n",
    "        else:\n",
    "            ignored_blocks.append(child)\n",
    "\n",
    "    return model_blocks, ignored_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5279f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mobilenet_blocks(model):\n",
    "\n",
    "    model_blocks, ignored_blocks = [], []\n",
    "\n",
    "    for feat in model.features:\n",
    "        if not isinstance(feat, nn.Sequential):\n",
    "            model_blocks.append(feat)\n",
    "        else:\n",
    "            ignored_blocks.append(feat)\n",
    "\n",
    "    ignored_blocks.append(model.classifier)\n",
    "\n",
    "    return model_blocks, ignored_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34106bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convnext_blocks(model):\n",
    "\n",
    "      model_blocks, ignored_blocks = [], []\n",
    "\n",
    "      for stage in model.stages:\n",
    "            for block in stage.blocks:\n",
    "                  if isinstance(block, nn.Module):\n",
    "                        model_blocks.append(block)\n",
    "            model_blocks.append(stage.downsample)\n",
    "\n",
    "      ignored_blocks.extend([model.stem, model.norm_pre, model.head])\n",
    "\n",
    "      return model_blocks, ignored_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74eb773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_blocks(model):\n",
    "    if model.__class__.__name__ == \"MobileNetV2\":\n",
    "        return get_mobilenet_blocks(model)\n",
    "    elif model.__class__.__name__ == \"ResNet\":\n",
    "        return get_resnet_blocks(model)\n",
    "    elif model.__class__.__name__ == \"ConvNeXt\":\n",
    "        return get_convnext_blocks(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8183c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "\n",
    "def selective_block_pruning(trained_model, prune_method, pruning_ratios, data_loader, device):\n",
    "    \n",
    "\n",
    "    model = copy.deepcopy(trained_model)\n",
    "    model_blocks, ignored_layers = get_model_blocks(model)\n",
    "    model.to(device)\n",
    "\n",
    "    pruning_info = {i: {\"block\": model_blocks[i], \"pruning_ratio\": ratio} for i, ratio in enumerate(pruning_ratios)}\n",
    "\n",
    "    if prune_method == \"Taylor\":\n",
    "        imp = tp.importance.TaylorImportance()\n",
    "\n",
    "        if isinstance(imp, (tp.importance.GroupTaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "            model.zero_grad()\n",
    "            if isinstance(imp, tp.importance.GroupHessianImportance):\n",
    "                imp.zero_grad() # clear the accumulated gradients\n",
    "            print(\"Accumulating gradients for pruning...\")\n",
    "            for k, (imgs, lbls) in enumerate(tqdm(data_loader)):\n",
    "                if k>=taylor_batchs: break\n",
    "                imgs = imgs.to(device)\n",
    "                lbls = lbls.to(device)\n",
    "                output = model(imgs)\n",
    "                if isinstance(imp, tp.importance.GroupHessianImportance): # per-sample gradients for hessian\n",
    "                    loss = torch.nn.functional.cross_entropy(output, lbls, reduction='none')\n",
    "                    for l in loss:\n",
    "                        model.zero_grad()\n",
    "                        l.backward(retain_graph=True)\n",
    "                        imp.accumulate_grad(model) # accumulate gradients\n",
    "                elif isinstance(imp, tp.importance.GroupTaylorImportance): # batch gradients for first-order taylor\n",
    "                    loss = torch.nn.functional.cross_entropy(output, lbls)\n",
    "                    loss.backward()\n",
    "\n",
    "    elif prune_method == \"Magnitude\":\n",
    "        imp = tp.importance.MagnitudeImportance()\n",
    "\n",
    "    else:\n",
    "        warnings.warn(f\"Invalid pruning method: '{prune_method}'. Expected 'Taylor' or 'Magnitude'.\", UserWarning)\n",
    "        raise ValueError(\"Pruning method must be either 'Taylor' or 'Magnitude'.\")\n",
    "\n",
    "    _, original_nparams = tp.utils.count_ops_and_params(model, imgs)\n",
    "\n",
    "    for i, info in pruning_info.items():\n",
    "        _, pruning_ratio = info[\"block\"], info[\"pruning_ratio\"]\n",
    "        if pruning_ratio == 0.0:\n",
    "            continue\n",
    "\n",
    "        ignored_layers_block = [pruning_info[j][\"block\"] for j in range(len(pruning_info)) if j != i]\n",
    "        combined_ignored_layers = ignored_layers + ignored_layers_block\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            pruner_group = tp.pruner.MagnitudePruner(\n",
    "                model,\n",
    "                example_inputs=imgs,\n",
    "                importance=imp,\n",
    "                pruning_ratio=pruning_ratio,\n",
    "                ignored_layers=combined_ignored_layers,\n",
    "            )\n",
    "            pruner_group.step()\n",
    "\n",
    "            macs, nparams = tp.utils.count_ops_and_params(model, imgs)\n",
    "            if original_nparams - nparams == 0:\n",
    "                count += 1\n",
    "                if count > 1:\n",
    "                    break\n",
    "                pruning_ratio = 0.5\n",
    "\n",
    "            original_nparams = nparams\n",
    "\n",
    "    del imgs, lbls, output\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    return model, macs, nparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d7712cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def perplexity_analysis_with_contributions(original_model, prune_method, data_loader, device):\n",
    "\n",
    "    \n",
    "    model_blocks, ignored_layers = get_model_blocks(original_model)\n",
    "    blocks_number = len(model_blocks)\n",
    "\n",
    "    total_block_accuracy = [0.0 for _ in range(blocks_number)]\n",
    "    params_reduction = []\n",
    "    macs_reduction = []\n",
    "\n",
    "    # logging.info(\"\\n=== Computing baseline accuracy without block replacement ===\")\n",
    "    print(\"\\n=== Computing baseline accuracy without block replacement ===\")\n",
    "    baseline_accuracy, baseline_loss = validate_model(original_model, data_loader, device='cpu')\n",
    "    # logging.info(f\"Baseline accuracy: {baseline_accuracy*100:.2f}%\")\n",
    "    print(f\"Baseline accuracy: {baseline_accuracy*100:.2f}%\")\n",
    "    \n",
    "    input_size = [3, 224, 224]\n",
    "    example_inputs = torch.randn(1, *input_size)\n",
    "    original_macs, original_nparams = tp.utils.count_ops_and_params(original_model, example_inputs)\n",
    "\n",
    "\n",
    "    for block_idx in range(blocks_number):\n",
    "\n",
    "        # logging.info(\"\\n=== Replacing Blocks and Tracking Reductions ===\")\n",
    "        print(\"\\n=== Replacing Blocks and Tracking Reductions ===\")\n",
    "\n",
    "        pruning_ratios = (np.eye(blocks_number) * 0.8)[block_idx]\n",
    "\n",
    "        pruned_model, macs, nparams = selective_block_pruning(\n",
    "        original_model, prune_method, pruning_ratios, data_loader, device\n",
    "        )\n",
    "\n",
    "        # logging.info(f\"\\nReplacing Block {block_idx}:\")\n",
    "        # logging.info(f\"  - MACs Reduction: {((original_macs - macs) / original_macs * 100):.2f}%\")\n",
    "        # logging.info(f\"  - Parameters Reduction: {((original_nparams - nparams)/original_nparams*100):.2f}%\")\n",
    "        print(f\"\\nReplacing Block {block_idx}:\")\n",
    "        print(f\"  - MACs Reduction: {((original_macs - macs) / original_macs * 100):.2f}%\")\n",
    "        print(f\"  - Parameters Reduction: {((original_nparams - nparams)/original_nparams*100):.2f}%\")\n",
    "        \n",
    "\n",
    "        params_reduction.append(original_nparams - nparams)\n",
    "        macs_reduction.append(original_macs - macs)\n",
    "\n",
    "        pruned_model.to(device)\n",
    "        pruned_model.eval()\n",
    "        \n",
    "        pruned_accuracy,_ = validate_model(pruned_model, data_loader, device='cpu')\n",
    "\n",
    "        del pruned_model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # logging.info(f\"Accuracy After Pruning This Block: {pruned_accuracy*100:.2f}%\\n\")\n",
    "        print(f\"Accuracy After Pruning This Block: {pruned_accuracy*100:.2f}%\\n\")\n",
    "\n",
    "        total_block_accuracy[block_idx] += pruned_accuracy\n",
    "\n",
    "    total_accuracy_reduction = 0.0\n",
    "    block_reductions = []\n",
    "    total_params_reduction = 0.0\n",
    "    total_macs_reduction = 0.0\n",
    "\n",
    "    for block_idx in range(blocks_number):\n",
    "        final_average_accuracy = total_block_accuracy[block_idx]\n",
    "        accuracy_reduction = baseline_accuracy - final_average_accuracy\n",
    "        if accuracy_reduction < 0:\n",
    "            print(f\"Block {block_idx} improved accuracy by {-accuracy_reduction*100:.2f}% — treated as 0 for importance.\")\n",
    "            accuracy_reduction = 0.0\n",
    "        block_reductions.append(accuracy_reduction)\n",
    "        total_accuracy_reduction += accuracy_reduction\n",
    "        total_params_reduction += params_reduction[block_idx]\n",
    "        total_macs_reduction += macs_reduction[block_idx] \n",
    "\n",
    "    weighted_importance_scores = []\n",
    "\n",
    "    # logging.info(\"\\n=== Relative Contribution of Each Block ===\")\n",
    "    print(\"\\n=== Relative Contribution of Each Block ===\")\n",
    "    for block_idx in range(blocks_number):\n",
    "        if total_accuracy_reduction == 0:\n",
    "            total_accuracy_reduction = 1e-8  # avoid division by zero\n",
    "        relative_contribution_accuracy = (block_reductions[block_idx] / total_accuracy_reduction) * 100\n",
    "        relative_contribution_params = (1 - (params_reduction[block_idx] / total_params_reduction)) * 100\n",
    "        relative_contribution_macs = (1 - (macs_reduction[block_idx] / total_macs_reduction)) * 100\n",
    "\n",
    "        weight_accuracy = 0.5\n",
    "        weight_params = 0.3\n",
    "        weight_macs = 0.2\n",
    "        weighted_importance = (weight_accuracy * relative_contribution_accuracy) \n",
    "        + (weight_params * relative_contribution_params) \n",
    "        + (weight_macs * relative_contribution_macs)\n",
    "\n",
    "        # logging.info(\n",
    "        # f\"Block {block_idx}: Accuracy Decrease Contribution = {relative_contribution_accuracy:.2f}%, \"\n",
    "        # f\"Parameter Reduction = {100 - relative_contribution_params:.2f}%, \"\n",
    "        # f\"MACs Reduction = {100 - relative_contribution_macs:.2f}%, \"\n",
    "        # f\"Weighted Importance Score = {weighted_importance:.2f}\"\n",
    "        # )  \n",
    "        print(f\"Block {block_idx}: Accuracy Decrease Contribution = {relative_contribution_accuracy:.2f}%, \")\n",
    "        print(f\"Parameter Reduction = {100 - relative_contribution_params:.2f}%, \")\n",
    "        print(f\"MACs Reduction = {100 - relative_contribution_macs:.2f}%, \")\n",
    "        print(f\"Weighted Importance Score = {weighted_importance:.2f}\") \n",
    "\n",
    "        epsilon = 1e-1\n",
    "        if (abs(100 - relative_contribution_params) < epsilon) and (abs(100 - relative_contribution_macs) < epsilon):\n",
    "            weighted_importance = -1\n",
    "            print(f\"Block {block_idx} is unprunable\")\n",
    "\n",
    "        weighted_importance_scores.append(weighted_importance)\n",
    "\n",
    "    return weighted_importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "624deb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def perplexity_analysis_with_contributions(original_model, prune_method, data_loader, device):\n",
    "\n",
    "    \n",
    "#     model_blocks, ignored_layers = get_model_blocks(original_model)\n",
    "#     blocks_number = len(model_blocks)\n",
    "\n",
    "#     total_block_accuracy = [0.0 for _ in range(blocks_number)]\n",
    "#     params_reduction = []\n",
    "#     macs_reduction = []\n",
    "\n",
    "#     # logging.info(\"\\n=== Computing baseline accuracy without block replacement ===\")\n",
    "#     print(\"\\n=== Computing baseline accuracy without block replacement ===\")\n",
    "#     baseline_accuracy, baseline_loss = validate_model(original_model, data_loader, device='cpu')\n",
    "#     # logging.info(f\"Baseline accuracy: {baseline_accuracy*100:.2f}%\")\n",
    "#     print(f\"Baseline accuracy: {baseline_accuracy*100:.2f}%\")\n",
    "    \n",
    "#     input_size = [3, 224, 224]\n",
    "#     example_inputs = torch.randn(1, *input_size).to(device)\n",
    "#     original_macs, original_nparams = tp.utils.count_ops_and_params(original_model, example_inputs)\n",
    "\n",
    "\n",
    "#     # logging.info(\"\\n=== Replacing Blocks and Tracking Reductions ===\")\n",
    "#     print(\"\\n=== Replacing Blocks and Tracking Reductions ===\")\n",
    "\n",
    "#     # pruning_ratios = (np.eye(blocks_number) * 0.8)[block_idx]\n",
    "\n",
    "#     pruned_acc, pruned_macs, pruned_params = selective_block_pruning(\n",
    "#     original_model, prune_method, data_loader, device\n",
    "#     )\n",
    "\n",
    "#     # logging.info(f\"\\nReplacing Block {block_idx}:\")\n",
    "#     # logging.info(f\"  - MACs Reduction: {((original_macs - macs) / original_macs * 100):.2f}%\")\n",
    "#     # logging.info(f\"  - Parameters Reduction: {((original_nparams - nparams)/original_nparams*100):.2f}%\")\n",
    "\n",
    "#     params_reduction = [original_nparams - param for param in pruned_params]\n",
    "#     macs_reduction = [original_macs - macs for macs in pruned_macs]\n",
    "\n",
    "#     # pruned_model.to(device)\n",
    "#     # pruned_model.eval()\n",
    "    \n",
    "#     # pruned_accuracy,_ = validate_model(pruned_model, data_loader, device='cpu')\n",
    "\n",
    "#     # del pruned_model\n",
    "#     # torch.cuda.empty_cache()\n",
    "\n",
    "#     # logging.info(f\"Accuracy After Pruning This Block: {nacc*100:.2f}%\\n\")\n",
    "#     total_block_accuracy = pruned_acc\n",
    "\n",
    "#     total_accuracy_reduction = 0.0\n",
    "#     block_reductions = []\n",
    "#     total_params_reduction = 0.0\n",
    "#     total_macs_reduction = 0.0\n",
    "\n",
    "#     for block_idx in range(blocks_number):\n",
    "#         final_average_accuracy = total_block_accuracy[block_idx]\n",
    "#         accuracy_reduction = baseline_accuracy - final_average_accuracy\n",
    "#         block_reductions.append(accuracy_reduction)\n",
    "#         total_accuracy_reduction += accuracy_reduction\n",
    "#         total_params_reduction += params_reduction[block_idx]\n",
    "#         total_macs_reduction += macs_reduction[block_idx] \n",
    "\n",
    "#     relative_contributions = []\n",
    "#     weighted_importance_scores = []\n",
    "\n",
    "#     # logging.info(\"\\n=== Relative Contribution of Each Block ===\")\n",
    "#     print(\"\\n=== Relative Contribution of Each Block ===\")\n",
    "#     for block_idx in range(blocks_number):\n",
    "#         relative_contribution_accuracy = (block_reductions[block_idx] / total_accuracy_reduction) * 100\n",
    "#         relative_contribution_params = (1 - (params_reduction[block_idx] / total_params_reduction)) * 100\n",
    "#         relative_contribution_macs = (1 - (macs_reduction[block_idx] / total_macs_reduction)) * 100\n",
    "\n",
    "#         weight_accuracy = 0.5\n",
    "#         weight_params = 0.3\n",
    "#         weight_macs = 0.2\n",
    "#         weighted_importance = (weight_accuracy * relative_contribution_accuracy) \n",
    "#         + (weight_params * relative_contribution_params) \n",
    "#         + (weight_macs * relative_contribution_macs)\n",
    "\n",
    "#         logging.info(\n",
    "#         f\"Block {block_idx}: Accuracy Decrease Contribution = {relative_contribution_accuracy:.2f}%, \"\n",
    "#         f\"Parameter Reduction = {100 - relative_contribution_params:.2f}%, \"\n",
    "#         f\"MACs Reduction = {100 - relative_contribution_macs:.2f}%, \"\n",
    "#         f\"Weighted Importance Score = {weighted_importance:.2f}\"\n",
    "#         )  \n",
    "#         print(f\"Block {block_idx}: Accuracy Decrease Contribution = {relative_contribution_accuracy:.2f}%, \")\n",
    "#         print(f\"Parameter Reduction = {100 - relative_contribution_params:.2f}%, \")\n",
    "#         print(f\"MACs Reduction = {100 - relative_contribution_macs:.2f}%, \")\n",
    "#         print(f\"Weighted Importance Score = {weighted_importance:.2f}\")\n",
    "\n",
    "#         relative_contributions.append(relative_contribution_accuracy)\n",
    "#         weighted_importance_scores.append(weighted_importance)\n",
    "\n",
    "#     return weighted_importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4c1deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(trained_model, prune_method, pruning_ratios, data_loader, device):\n",
    "   \n",
    "    # Make a copy of the trained model\n",
    "    model = copy.deepcopy(trained_model)\n",
    "    model.to(device)\n",
    "\n",
    "    model_blocks, ignored_layers = get_model_blocks(model)\n",
    "\n",
    "\n",
    "    pruning_info = {i: {\"block\": model_blocks[i], \"pruning_ratio\": ratio} \n",
    "                    for i, ratio in enumerate(pruning_ratios)}\n",
    "\n",
    "    if prune_method == 'Taylor':\n",
    "        imp = tp.importance.TaylorImportance() \n",
    "        \n",
    "        if isinstance(imp, (tp.importance.GroupTaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "            model.zero_grad()\n",
    "            if isinstance(imp, tp.importance.GroupHessianImportance):\n",
    "                imp.zero_grad() # clear the accumulated gradients\n",
    "            print(\"Accumulating gradients for pruning...\")\n",
    "            for k, (imgs, lbls) in enumerate(tqdm(data_loader)):\n",
    "                if k>=taylor_batchs: break\n",
    "                imgs = imgs.to(device)\n",
    "                lbls = lbls.to(device)\n",
    "                output = model(imgs)\n",
    "                if isinstance(imp, tp.importance.GroupHessianImportance): # per-sample gradients for hessian\n",
    "                    loss = torch.nn.functional.cross_entropy(output, lbls, reduction='none')\n",
    "                    for l in loss:\n",
    "                        model.zero_grad()\n",
    "                        l.backward(retain_graph=True)\n",
    "                        imp.accumulate_grad(model) # accumulate gradients\n",
    "                elif isinstance(imp, tp.importance.GroupTaylorImportance): # batch gradients for first-order taylor\n",
    "                    loss = torch.nn.functional.cross_entropy(output, lbls)\n",
    "                    loss.backward()\n",
    "\n",
    "        original_macs, original_params = tp.utils.count_ops_and_params(model, imgs)\n",
    "\n",
    "        # Prune each block while ignoring other layers\n",
    "        for i, info in pruning_info.items():\n",
    "            pruning_ratio = info[\"pruning_ratio\"]\n",
    "            \n",
    "            # Add all blocks to the ignored layers except the block being pruned\n",
    "            ignored_layers_block = [pruning_info[j][\"block\"] for j in range(len(pruning_info)) if j != i]\n",
    "\n",
    "            # Combine fixed ignored layers (conv_stem, bn1, classifier) with the ignored blocks\n",
    "            combined_ignored_layers = ignored_layers + ignored_layers_block\n",
    "\n",
    "            # Apply pruning using the combined ignored layers\n",
    "            pruner_group = tp.pruner.MagnitudePruner( \n",
    "                model,\n",
    "                example_inputs=imgs,\n",
    "                importance=imp,\n",
    "                pruning_ratio=pruning_ratio,\n",
    "                ignored_layers=combined_ignored_layers,\n",
    "                iterative_steps=1,\n",
    "            )\n",
    "\n",
    "            # Step through pruning\n",
    "            pruner_group.step()\n",
    "\n",
    "    # Counting MACs and Params after pruning\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, imgs)\n",
    "\n",
    "    # logging.info(f\"MACs of the Pruned Model: {macs/ 1e9} G\")\n",
    "    print(f\"MACs of the Original Model: {original_macs/ 1e9} G  -->  MACs of the Pruned Model: {macs/ 1e9} G\")\n",
    "    # logging.info(f\"# Parameters of the Pruned Model: {nparams/ 1e3} K\")\n",
    "    print(f\"# Parameters of the Original Model: {original_params/ 1e3} K  --> # Parameters of the Pruned Model: {nparams/ 1e3} K\")\n",
    "\n",
    "    param_reduction = ((original_params - nparams) / original_params) * 100\n",
    "    macs_reduction = ((original_macs - macs) / original_macs) * 100\n",
    "\n",
    "    # Free up GPU memory\n",
    "    del imgs, lbls, output\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, math.ceil(param_reduction), math.ceil(macs_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43909cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pruning_ratios_intense(contributions, max_pruning_ratio=0.9, k=5):\n",
    "    # Normalize the contributions to get values between 0 and 1\n",
    "    # total_contribution = sum(contributions)\n",
    "    # Ignore the unprunable blocks\n",
    "    total_contribution = sum([contribution for contribution in contributions if contribution != -1])\n",
    "\n",
    "    normalized_contributions = [contribution / total_contribution if contribution != -1 else contribution for contribution in contributions]\n",
    "\n",
    "    # Apply exponential decay to magnify the effect for less important blocks\n",
    "    # Assign zero pruning factor for unprunable blocks\n",
    "    pruning_factors = [np.exp(-k * nc) if nc != -1 else 0 for nc in normalized_contributions]\n",
    "\n",
    "    # Normalize the pruning factors so they stay within the max pruning ratio\n",
    "    max_factor = max(pruning_factors)\n",
    "    normalized_factors = [pf / max_factor for pf in pruning_factors]\n",
    "\n",
    "    # Scale by the maximum pruning ratio\n",
    "    pruning_ratios = [max_pruning_ratio * nf for nf in normalized_factors]\n",
    "\n",
    "    pruning_ratios = [round(num, 2) for num in pruning_ratios]\n",
    "\n",
    "    return pruning_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78ac8bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Computing baseline accuracy without block replacement ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 43.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 69.97%\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 87.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 0:\n",
      "  - MACs Reduction: 7.64%\n",
      "  - Parameters Reduction: 0.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:10<00:00, 47.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 0.29%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 90.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 1:\n",
      "  - MACs Reduction: 10.42%\n",
      "  - Parameters Reduction: 0.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:08<00:00, 62.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 0.10%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 90.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 2:\n",
      "  - MACs Reduction: 8.82%\n",
      "  - Parameters Reduction: 0.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:10<00:00, 48.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 0.24%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 90.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 3:\n",
      "  - MACs Reduction: 5.32%\n",
      "  - Parameters Reduction: 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 46.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 0.20%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 88.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 4:\n",
      "  - MACs Reduction: 3.69%\n",
      "  - Parameters Reduction: 0.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 45.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 15.97%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 98.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 5:\n",
      "  - MACs Reduction: 3.69%\n",
      "  - Parameters Reduction: 0.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 46.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 29.39%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 91.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 6:\n",
      "  - MACs Reduction: 2.53%\n",
      "  - Parameters Reduction: 0.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 41.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 0.10%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 89.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 7:\n",
      "  - MACs Reduction: 3.35%\n",
      "  - Parameters Reduction: 1.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 40.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 3.61%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 96.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 8:\n",
      "  - MACs Reduction: 3.35%\n",
      "  - Parameters Reduction: 1.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 41.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 62.74%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 97.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 9:\n",
      "  - MACs Reduction: 3.35%\n",
      "  - Parameters Reduction: 1.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 50.15%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 97.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 10:\n",
      "  - MACs Reduction: 4.10%\n",
      "  - Parameters Reduction: 1.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 41.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 0.15%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 89.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 11:\n",
      "  - MACs Reduction: 7.29%\n",
      "  - Parameters Reduction: 3.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 40.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 22.95%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 91.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 12:\n",
      "  - MACs Reduction: 7.29%\n",
      "  - Parameters Reduction: 3.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 41.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 8.94%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 95.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 13:\n",
      "  - MACs Reduction: 5.00%\n",
      "  - Parameters Reduction: 4.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 39.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 0.15%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 89.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 14:\n",
      "  - MACs Reduction: 4.92%\n",
      "  - Parameters Reduction: 9.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 41.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 18.70%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 92.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 15:\n",
      "  - MACs Reduction: 4.92%\n",
      "  - Parameters Reduction: 9.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 41.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 11.23%\n",
      "\n",
      "\n",
      "=== Replacing Blocks and Tracking Reductions ===\n",
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:05<00:00, 88.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing Block 16:\n",
      "  - MACs Reduction: 13.53%\n",
      "  - Parameters Reduction: 25.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:12<00:00, 41.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Pruning This Block: 0.24%\n",
      "\n",
      "\n",
      "=== Relative Contribution of Each Block ===\n",
      "Block 0: Accuracy Decrease Contribution = 7.23%, \n",
      "Parameter Reduction = 0.09%, \n",
      "MACs Reduction = 7.70%, \n",
      "Weighted Importance Score = 3.61\n",
      "Block 1: Accuracy Decrease Contribution = 7.25%, \n",
      "Parameter Reduction = 0.23%, \n",
      "MACs Reduction = 10.50%, \n",
      "Weighted Importance Score = 3.62\n",
      "Block 2: Accuracy Decrease Contribution = 7.23%, \n",
      "Parameter Reduction = 0.39%, \n",
      "MACs Reduction = 8.89%, \n",
      "Weighted Importance Score = 3.62\n",
      "Block 3: Accuracy Decrease Contribution = 7.24%, \n",
      "Parameter Reduction = 0.45%, \n",
      "MACs Reduction = 5.36%, \n",
      "Weighted Importance Score = 3.62\n",
      "Block 4: Accuracy Decrease Contribution = 5.60%, \n",
      "Parameter Reduction = 0.66%, \n",
      "MACs Reduction = 3.72%, \n",
      "Weighted Importance Score = 2.80\n",
      "Block 5: Accuracy Decrease Contribution = 4.21%, \n",
      "Parameter Reduction = 0.66%, \n",
      "MACs Reduction = 3.72%, \n",
      "Weighted Importance Score = 2.10\n",
      "Block 6: Accuracy Decrease Contribution = 7.25%, \n",
      "Parameter Reduction = 0.94%, \n",
      "MACs Reduction = 2.55%, \n",
      "Weighted Importance Score = 3.62\n",
      "Block 7: Accuracy Decrease Contribution = 6.88%, \n",
      "Parameter Reduction = 2.44%, \n",
      "MACs Reduction = 3.38%, \n",
      "Weighted Importance Score = 3.44\n",
      "Block 8: Accuracy Decrease Contribution = 0.75%, \n",
      "Parameter Reduction = 2.44%, \n",
      "MACs Reduction = 3.38%, \n",
      "Weighted Importance Score = 0.37\n",
      "Block 9: Accuracy Decrease Contribution = 2.06%, \n",
      "Parameter Reduction = 2.44%, \n",
      "MACs Reduction = 3.38%, \n",
      "Weighted Importance Score = 1.03\n",
      "Block 10: Accuracy Decrease Contribution = 7.24%, \n",
      "Parameter Reduction = 2.99%, \n",
      "MACs Reduction = 4.14%, \n",
      "Weighted Importance Score = 3.62\n",
      "Block 11: Accuracy Decrease Contribution = 4.88%, \n",
      "Parameter Reduction = 5.32%, \n",
      "MACs Reduction = 7.34%, \n",
      "Weighted Importance Score = 2.44\n",
      "Block 12: Accuracy Decrease Contribution = 6.33%, \n",
      "Parameter Reduction = 5.32%, \n",
      "MACs Reduction = 7.34%, \n",
      "Weighted Importance Score = 3.16\n",
      "Block 13: Accuracy Decrease Contribution = 7.24%, \n",
      "Parameter Reduction = 6.98%, \n",
      "MACs Reduction = 5.04%, \n",
      "Weighted Importance Score = 3.62\n",
      "Block 14: Accuracy Decrease Contribution = 5.32%, \n",
      "Parameter Reduction = 14.42%, \n",
      "MACs Reduction = 4.96%, \n",
      "Weighted Importance Score = 2.66\n",
      "Block 15: Accuracy Decrease Contribution = 6.09%, \n",
      "Parameter Reduction = 14.42%, \n",
      "MACs Reduction = 4.96%, \n",
      "Weighted Importance Score = 3.05\n",
      "Block 16: Accuracy Decrease Contribution = 7.23%, \n",
      "Parameter Reduction = 39.82%, \n",
      "MACs Reduction = 13.64%, \n",
      "Weighted Importance Score = 3.62\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "relative_contribution = perplexity_analysis_with_contributions(model, 'Taylor', cal_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82813e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ConvNext Blocks Contributions\n",
    "\n",
    "# relative_contribution = [30.936995153473347,\n",
    "#  2.827140549273021,\n",
    "#  5.573505654281099,\n",
    "#  -1,\n",
    "#  1.2924071082390953,\n",
    "#  1.050080775444265,\n",
    "#  0.48465266558966075,\n",
    "#  -1,\n",
    "#  0.0,\n",
    "#  0.5654281098546041,\n",
    "#  0.5654281098546041,\n",
    "#  0.40387722132471726,\n",
    "#  0.24232633279483037,\n",
    "#  0.6462035541195477,\n",
    "#  0.16155088852988692,\n",
    "#  0.24232633279483037,\n",
    "#  0.6462035541195477,\n",
    "#  0.0,\n",
    "#  0.24232633279483037,\n",
    "#  0.48465266558966075,\n",
    "#  0.08077544426494346,\n",
    "#  0.32310177705977383,\n",
    "#  0.16155088852988692,\n",
    "#  0.08077544426494346,\n",
    "#  0.16155088852988692,\n",
    "#  0.0,\n",
    "#  0.40387722132471726,\n",
    "#  0.40387722132471726,\n",
    "#  0.24232633279483037,\n",
    "#  0.08077544426494346,\n",
    "#  0.5654281098546041,\n",
    "#  0.0,\n",
    "#  0.0,\n",
    "#  0.0,\n",
    "#  0.24232633279483037,\n",
    "#  -1,\n",
    "#  0.0,\n",
    "#  0.40387722132471726,\n",
    "#  0.48465266558966075,\n",
    "#  -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet Blocks Contributions\n",
    "\n",
    "# relative_contribution = [6.866310160427807,\n",
    "#  0.1711229946524064,\n",
    "#  0.6631016042780749,\n",
    "#  11.336898395721926,\n",
    "#  1.0053475935828877,\n",
    "#  0.4598930481283422,\n",
    "#  3.3262032085561497,\n",
    "#  16.06417112299465,\n",
    "#  0.0,\n",
    "#  0.09625668449197862,\n",
    "#  0.0320855614973262,\n",
    "#  0.2352941176470588,\n",
    "#  0.0427807486631016,\n",
    "#  0.1497326203208556,\n",
    "#  0.27807486631016043,\n",
    "#  0.2566844919786096,\n",
    "#  0.19251336898395724,\n",
    "#  0.35294117647058826,\n",
    "#  0.2994652406417112,\n",
    "#  0.1497326203208556,\n",
    "#  0.0855614973262032,\n",
    "#  0.1176470588235294,\n",
    "#  0.1176470588235294,\n",
    "#  0.0,\n",
    "#  0.2887700534759358,\n",
    "#  0.0641711229946524,\n",
    "#  0.2566844919786096,\n",
    "#  0.4812834224598931,\n",
    "#  0.41711229946524064,\n",
    "#  0.5454545454545455,\n",
    "#  3.048128342245989,\n",
    "#  1.6363636363636365,\n",
    "#  0.9625668449197862]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MobileNet Blocks Contributions\n",
    "\n",
    "# relative_contribution = [3.593584505976698,\n",
    "#  3.618802642860745,\n",
    "#  3.5986281333535075,\n",
    "#  3.62132445654915,\n",
    "#  2.6327734906945075,\n",
    "#  2.0628435971150454,\n",
    "#  3.618802642860745,\n",
    "#  3.4624501941796537,\n",
    "#  0.5094063650577496,\n",
    "#  0.9633328289705957,\n",
    "#  3.618802642860745,\n",
    "#  2.4839864830786302,\n",
    "#  3.1573107378826855,\n",
    "#  3.6288898976143646,\n",
    "#  2.6932970192162204,\n",
    "#  3.1119180914914004,\n",
    "#  3.6238462702375545]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_ratios = calculate_pruning_ratios_intense(relative_contribution, max_pruning_ratio=0.99, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d60552d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99),\n",
       " np.float64(0.99)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a21c32e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:06<00:00, 82.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs of the Original Model: 0.320236538 G  -->  MACs of the Pruned Model: 0.061527024 G\n",
      "# Parameters of the Original Model: 3504.872 K  --> # Parameters of the Pruned Model: 1309.808 K\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pruned_model, param_reduction, macs_reduction = prune_model(model, 'Taylor', pruning_ratios, cal_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4df63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_accuracy,_ = validate_model(pruned_model, val_loader, device='cuda')\n",
    "pruned_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2d2c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model, f'/home/ict317-3/Mohammad/Isomorphic-Pruning/our_pruned_models/MobileNet/pruned_model_{param_reduction}%.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d10fd7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs = 3.563852603 G, and parameters count = 18.574907 M\n"
     ]
    }
   ],
   "source": [
    "input_size = [3, 224, 224]\n",
    "example_inputs = torch.randn(1, *input_size, device=device)\n",
    "\n",
    "model = torch.load(\"/home/ict317-3/Mohammad/Isomorphic-Pruning/our_pruned_models/ConvNext4.2/pruned_model_80%.pth\", weights_only=False)\n",
    "model.to(device)\n",
    "\n",
    "macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "print(f\"MACs = {macs/1e9} G, and parameters count = {nparams/1e6} M\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
