{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c267e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch_pruning as tp\n",
    "\n",
    "import pbench\n",
    "pbench.forward_patch.patch_timm_forward() # patch timm.forward() to support pruning\n",
    "\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import torchvision as tv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e85d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "def prepare_imagenet(imagenet_root, train_batch_size=64, val_batch_size=128, num_workers=4, use_imagenet_mean_std=True, interpolation='bicubic', val_resize=256):\n",
    "    \"\"\"The imagenet_root should contain train and val folders.\n",
    "    \"\"\"\n",
    "    interpolation = getattr(T.InterpolationMode, interpolation.upper())\n",
    "\n",
    "    print('Parsing dataset...')\n",
    "    train_dst = ImageFolder(os.path.join(imagenet_root, 'train'), \n",
    "                            transform=pbench.data.presets.ClassificationPresetEval(\n",
    "                                mean=[0.485, 0.456, 0.406] if use_imagenet_mean_std else [0.5, 0.5, 0.5],\n",
    "                                std=[0.229, 0.224, 0.225] if use_imagenet_mean_std else [0.5, 0.5, 0.5],\n",
    "                                crop_size=224,\n",
    "                                resize_size=val_resize,\n",
    "                                interpolation=interpolation,\n",
    "                            )\n",
    "    )\n",
    "    val_dst = ImageFolder(os.path.join(imagenet_root, 'val'), \n",
    "                          transform=pbench.data.presets.ClassificationPresetEval(\n",
    "                                mean=[0.485, 0.456, 0.406] if use_imagenet_mean_std else [0.5, 0.5, 0.5],\n",
    "                                std=[0.229, 0.224, 0.225] if use_imagenet_mean_std else [0.5, 0.5, 0.5],\n",
    "                                crop_size=224,\n",
    "                                resize_size=val_resize,\n",
    "                                interpolation=interpolation,\n",
    "                            )\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dst, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dst, batch_size=val_batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    cal_subset_size = 1024\n",
    "    cal_indices = random.sample(range(len(val_loader)*val_batch_size), cal_subset_size)\n",
    "    cal_dst = Subset(val_dst, cal_indices)\n",
    "    cal_loader = torch.utils.data.DataLoader(cal_dst, batch_size=val_batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, val_loader, cal_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17d297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for k, (images, labels) in enumerate(tqdm(val_loader)):  \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss += torch.nn.functional.cross_entropy(outputs, labels, reduction='sum').item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    del images, outputs, predicted\n",
    "    gc.collect()\n",
    "    return correct / len(val_loader.dataset), loss / len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b0efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path ='data/imagenet'\n",
    "train_batch_size = 64\n",
    "val_batch_size = 16\n",
    "no_imagenet_mean_std = False\n",
    "val_resize = 256\n",
    "interpolation = 'bicubic' #'bilinear' \n",
    "\n",
    "model = 'convnext_base.fb_in1k' #'resnet101.tv_in1k' #'mobilenet_v2' \n",
    "is_torchvision = False\n",
    "drop = 0.0\n",
    "drop_path = 0.0\n",
    "ckpt = None\n",
    "\n",
    "taylor_batchs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c6485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing dataset...\n",
      "Loading timm model convnext_base.fb_in1k...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "example_inputs = torch.randn(1,3,224,224)\n",
    "train_loader, val_loader, cal_loader = prepare_imagenet(data_path, train_batch_size=train_batch_size, val_batch_size=val_batch_size, use_imagenet_mean_std= not no_imagenet_mean_std, val_resize=val_resize, interpolation=interpolation)\n",
    "\n",
    "if is_torchvision:\n",
    "        import torchvision\n",
    "        print(f\"Loading torchvision model {model}...\")\n",
    "        model = torchvision.models.__dict__[model](pretrained=True).eval()\n",
    "else:\n",
    "    print(f\"Loading timm model {model}...\")\n",
    "    model = timm.create_model(model, pretrained=True, drop_rate=drop, drop_path_rate=drop_path).eval()\n",
    "\n",
    "if ckpt is not None:\n",
    "    print(f\"Loading checkpoint from {ckpt}...\")\n",
    "    ckpt = torch.load(ckpt, map_location='cpu')\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab19ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_blocks(model):\n",
    "\n",
    "    model_blocks, ignored_blocks = [], []\n",
    "\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            model_blocks.extend([sub_child for sub_child in child.children()])\n",
    "        else:\n",
    "            ignored_blocks.append(child)\n",
    "\n",
    "    return model_blocks, ignored_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5279f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mobilenet_blocks(model):\n",
    "\n",
    "    model_blocks, ignored_blocks = [], []\n",
    "\n",
    "    for feat in model.features:\n",
    "        if not isinstance(feat, nn.Sequential):\n",
    "            model_blocks.append(feat)\n",
    "        else:\n",
    "            ignored_blocks.append(feat)\n",
    "\n",
    "    ignored_blocks.append(model.classifier)\n",
    "\n",
    "    return model_blocks, ignored_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34106bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convnext_blocks(model):\n",
    "\n",
    "      model_blocks, ignored_blocks = [], []\n",
    "\n",
    "      for stage in model.stages:\n",
    "            for block in stage.blocks:\n",
    "                  if isinstance(block, nn.Module):\n",
    "                        model_blocks.append(block)\n",
    "            model_blocks.append(stage.downsample)\n",
    "\n",
    "      ignored_blocks.extend([model.stem, model.norm_pre, model.head])\n",
    "\n",
    "      return model_blocks, ignored_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74eb773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_blocks(model):\n",
    "    if model.__class__.__name__ == \"MobileNetV2\":\n",
    "        return get_mobilenet_blocks(model)\n",
    "    elif model.__class__.__name__ == \"ResNet\":\n",
    "        return get_resnet_blocks(model)\n",
    "    elif model.__class__.__name__ == \"ConvNeXt\":\n",
    "        return get_convnext_blocks(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8183c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "\n",
    "def selective_block_pruning(trained_model, prune_method, pruning_ratios, data_loader, device):\n",
    "    \n",
    "\n",
    "    model = copy.deepcopy(trained_model)\n",
    "    model_blocks, ignored_layers = get_model_blocks(model)\n",
    "    model.to(device)\n",
    "\n",
    "    pruning_info = {i: {\"block\": model_blocks[i], \"pruning_ratio\": ratio} for i, ratio in enumerate(pruning_ratios)}\n",
    "\n",
    "    if prune_method == \"Taylor\":\n",
    "        imp = tp.importance.TaylorImportance()\n",
    "\n",
    "        if isinstance(imp, (tp.importance.GroupTaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "            model.zero_grad()\n",
    "            if isinstance(imp, tp.importance.GroupHessianImportance):\n",
    "                imp.zero_grad() # clear the accumulated gradients\n",
    "            print(\"Accumulating gradients for pruning...\")\n",
    "            for k, (imgs, lbls) in enumerate(tqdm(data_loader)):\n",
    "                if k>=taylor_batchs: break\n",
    "                imgs = imgs.to(device)\n",
    "                lbls = lbls.to(device)\n",
    "                output = model(imgs)\n",
    "                if isinstance(imp, tp.importance.GroupHessianImportance): # per-sample gradients for hessian\n",
    "                    loss = torch.nn.functional.cross_entropy(output, lbls, reduction='none')\n",
    "                    for l in loss:\n",
    "                        model.zero_grad()\n",
    "                        l.backward(retain_graph=True)\n",
    "                        imp.accumulate_grad(model) # accumulate gradients\n",
    "                elif isinstance(imp, tp.importance.GroupTaylorImportance): # batch gradients for first-order taylor\n",
    "                    loss = torch.nn.functional.cross_entropy(output, lbls)\n",
    "                    loss.backward()\n",
    "\n",
    "    elif prune_method == \"Magnitude\":\n",
    "        imp = tp.importance.MagnitudeImportance()\n",
    "\n",
    "    else:\n",
    "        warnings.warn(f\"Invalid pruning method: '{prune_method}'. Expected 'Taylor' or 'Magnitude'.\", UserWarning)\n",
    "        raise ValueError(\"Pruning method must be either 'Taylor' or 'Magnitude'.\")\n",
    "\n",
    "    _, original_nparams = tp.utils.count_ops_and_params(model, imgs)\n",
    "\n",
    "    for i, info in pruning_info.items():\n",
    "        _, pruning_ratio = info[\"block\"], info[\"pruning_ratio\"]\n",
    "        if pruning_ratio == 0.0:\n",
    "            continue\n",
    "\n",
    "        ignored_layers_block = [pruning_info[j][\"block\"] for j in range(len(pruning_info)) if j != i]\n",
    "        combined_ignored_layers = ignored_layers + ignored_layers_block\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            pruner_group = tp.pruner.MagnitudePruner(\n",
    "                model,\n",
    "                example_inputs=imgs,\n",
    "                importance=imp,\n",
    "                pruning_ratio=pruning_ratio,\n",
    "                ignored_layers=combined_ignored_layers,\n",
    "            )\n",
    "            pruner_group.step()\n",
    "\n",
    "            macs, nparams = tp.utils.count_ops_and_params(model, imgs)\n",
    "            if original_nparams - nparams == 0:\n",
    "                count += 1\n",
    "                if count > 1:\n",
    "                    break\n",
    "                pruning_ratio = 0.5\n",
    "\n",
    "            original_nparams = nparams\n",
    "\n",
    "    del imgs, lbls, output\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    return model, macs, nparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d7712cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def perplexity_analysis_with_contributions(original_model, prune_method, data_loader, device):\n",
    "\n",
    "    \n",
    "    model_blocks, ignored_layers = get_model_blocks(original_model)\n",
    "    blocks_number = len(model_blocks)\n",
    "\n",
    "    total_block_accuracy = [0.0 for _ in range(blocks_number)]\n",
    "    params_reduction = []\n",
    "    macs_reduction = []\n",
    "\n",
    "    # logging.info(\"\\n=== Computing baseline accuracy without block replacement ===\")\n",
    "    print(\"\\n=== Computing baseline accuracy without block replacement ===\")\n",
    "    baseline_accuracy, baseline_loss = validate_model(original_model, data_loader, device='cpu')\n",
    "    # logging.info(f\"Baseline accuracy: {baseline_accuracy*100:.2f}%\")\n",
    "    print(f\"Baseline accuracy: {baseline_accuracy*100:.2f}%\")\n",
    "    \n",
    "    input_size = [3, 224, 224]\n",
    "    example_inputs = torch.randn(1, *input_size)\n",
    "    original_macs, original_nparams = tp.utils.count_ops_and_params(original_model, example_inputs)\n",
    "\n",
    "\n",
    "    for block_idx in range(blocks_number):\n",
    "\n",
    "        # logging.info(\"\\n=== Replacing Blocks and Tracking Reductions ===\")\n",
    "        print(\"\\n=== Replacing Blocks and Tracking Reductions ===\")\n",
    "\n",
    "        pruning_ratios = (np.eye(blocks_number) * 0.8)[block_idx]\n",
    "\n",
    "        pruned_model, macs, nparams = selective_block_pruning(\n",
    "        original_model, prune_method, pruning_ratios, data_loader, device\n",
    "        )\n",
    "\n",
    "        # logging.info(f\"\\nReplacing Block {block_idx}:\")\n",
    "        # logging.info(f\"  - MACs Reduction: {((original_macs - macs) / original_macs * 100):.2f}%\")\n",
    "        # logging.info(f\"  - Parameters Reduction: {((original_nparams - nparams)/original_nparams*100):.2f}%\")\n",
    "        print(f\"\\nReplacing Block {block_idx}:\")\n",
    "        print(f\"  - MACs Reduction: {((original_macs - macs) / original_macs * 100):.2f}%\")\n",
    "        print(f\"  - Parameters Reduction: {((original_nparams - nparams)/original_nparams*100):.2f}%\")\n",
    "        \n",
    "\n",
    "        params_reduction.append(original_nparams - nparams)\n",
    "        macs_reduction.append(original_macs - macs)\n",
    "\n",
    "        pruned_model.to(device)\n",
    "        pruned_model.eval()\n",
    "        \n",
    "        pruned_accuracy,_ = validate_model(pruned_model, data_loader, device='cpu')\n",
    "\n",
    "        del pruned_model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # logging.info(f\"Accuracy After Pruning This Block: {pruned_accuracy*100:.2f}%\\n\")\n",
    "        print(f\"Accuracy After Pruning This Block: {pruned_accuracy*100:.2f}%\\n\")\n",
    "\n",
    "        total_block_accuracy[block_idx] += pruned_accuracy\n",
    "\n",
    "    total_accuracy_reduction = 0.0\n",
    "    block_reductions = []\n",
    "    total_params_reduction = 0.0\n",
    "    total_macs_reduction = 0.0\n",
    "\n",
    "    for block_idx in range(blocks_number):\n",
    "        final_average_accuracy = total_block_accuracy[block_idx]\n",
    "        accuracy_reduction = baseline_accuracy - final_average_accuracy\n",
    "        if accuracy_reduction < 0:\n",
    "            print(f\"Block {block_idx} improved accuracy by {-accuracy_reduction*100:.2f}% — treated as 0 for importance.\")\n",
    "            accuracy_reduction = 0.0\n",
    "        block_reductions.append(accuracy_reduction)\n",
    "        total_accuracy_reduction += accuracy_reduction\n",
    "        total_params_reduction += params_reduction[block_idx]\n",
    "        total_macs_reduction += macs_reduction[block_idx] \n",
    "\n",
    "    weighted_importance_scores = []\n",
    "\n",
    "    # logging.info(\"\\n=== Relative Contribution of Each Block ===\")\n",
    "    print(\"\\n=== Relative Contribution of Each Block ===\")\n",
    "    for block_idx in range(blocks_number):\n",
    "        if total_accuracy_reduction == 0:\n",
    "            total_accuracy_reduction = 1e-8  # avoid division by zero\n",
    "        relative_contribution_accuracy = (block_reductions[block_idx] / total_accuracy_reduction) * 100\n",
    "        relative_contribution_params = (1 - (params_reduction[block_idx] / total_params_reduction)) * 100\n",
    "        relative_contribution_macs = (1 - (macs_reduction[block_idx] / total_macs_reduction)) * 100\n",
    "\n",
    "        weight_accuracy = 0.5\n",
    "        weight_params = 0.3\n",
    "        weight_macs = 0.2\n",
    "        weighted_importance = (weight_accuracy * relative_contribution_accuracy) \n",
    "        + (weight_params * relative_contribution_params) \n",
    "        + (weight_macs * relative_contribution_macs)\n",
    "\n",
    "        # logging.info(\n",
    "        # f\"Block {block_idx}: Accuracy Decrease Contribution = {relative_contribution_accuracy:.2f}%, \"\n",
    "        # f\"Parameter Reduction = {100 - relative_contribution_params:.2f}%, \"\n",
    "        # f\"MACs Reduction = {100 - relative_contribution_macs:.2f}%, \"\n",
    "        # f\"Weighted Importance Score = {weighted_importance:.2f}\"\n",
    "        # )  \n",
    "        print(f\"Block {block_idx}: Accuracy Decrease Contribution = {relative_contribution_accuracy:.2f}%, \")\n",
    "        print(f\"Parameter Reduction = {100 - relative_contribution_params:.2f}%, \")\n",
    "        print(f\"MACs Reduction = {100 - relative_contribution_macs:.2f}%, \")\n",
    "        print(f\"Weighted Importance Score = {weighted_importance:.2f}\") \n",
    "\n",
    "        weighted_importance_scores.append(weighted_importance)\n",
    "\n",
    "    return weighted_importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "624deb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def perplexity_analysis_with_contributions(original_model, prune_method, data_loader, device):\n",
    "\n",
    "    \n",
    "#     model_blocks, ignored_layers = get_model_blocks(original_model)\n",
    "#     blocks_number = len(model_blocks)\n",
    "\n",
    "#     total_block_accuracy = [0.0 for _ in range(blocks_number)]\n",
    "#     params_reduction = []\n",
    "#     macs_reduction = []\n",
    "\n",
    "#     # logging.info(\"\\n=== Computing baseline accuracy without block replacement ===\")\n",
    "#     print(\"\\n=== Computing baseline accuracy without block replacement ===\")\n",
    "#     baseline_accuracy, baseline_loss = validate_model(original_model, data_loader, device='cpu')\n",
    "#     # logging.info(f\"Baseline accuracy: {baseline_accuracy*100:.2f}%\")\n",
    "#     print(f\"Baseline accuracy: {baseline_accuracy*100:.2f}%\")\n",
    "    \n",
    "#     input_size = [3, 224, 224]\n",
    "#     example_inputs = torch.randn(1, *input_size).to(device)\n",
    "#     original_macs, original_nparams = tp.utils.count_ops_and_params(original_model, example_inputs)\n",
    "\n",
    "\n",
    "#     # logging.info(\"\\n=== Replacing Blocks and Tracking Reductions ===\")\n",
    "#     print(\"\\n=== Replacing Blocks and Tracking Reductions ===\")\n",
    "\n",
    "#     # pruning_ratios = (np.eye(blocks_number) * 0.8)[block_idx]\n",
    "\n",
    "#     pruned_acc, pruned_macs, pruned_params = selective_block_pruning(\n",
    "#     original_model, prune_method, data_loader, device\n",
    "#     )\n",
    "\n",
    "#     # logging.info(f\"\\nReplacing Block {block_idx}:\")\n",
    "#     # logging.info(f\"  - MACs Reduction: {((original_macs - macs) / original_macs * 100):.2f}%\")\n",
    "#     # logging.info(f\"  - Parameters Reduction: {((original_nparams - nparams)/original_nparams*100):.2f}%\")\n",
    "\n",
    "#     params_reduction = [original_nparams - param for param in pruned_params]\n",
    "#     macs_reduction = [original_macs - macs for macs in pruned_macs]\n",
    "\n",
    "#     # pruned_model.to(device)\n",
    "#     # pruned_model.eval()\n",
    "    \n",
    "#     # pruned_accuracy,_ = validate_model(pruned_model, data_loader, device='cpu')\n",
    "\n",
    "#     # del pruned_model\n",
    "#     # torch.cuda.empty_cache()\n",
    "\n",
    "#     # logging.info(f\"Accuracy After Pruning This Block: {nacc*100:.2f}%\\n\")\n",
    "#     total_block_accuracy = pruned_acc\n",
    "\n",
    "#     total_accuracy_reduction = 0.0\n",
    "#     block_reductions = []\n",
    "#     total_params_reduction = 0.0\n",
    "#     total_macs_reduction = 0.0\n",
    "\n",
    "#     for block_idx in range(blocks_number):\n",
    "#         final_average_accuracy = total_block_accuracy[block_idx]\n",
    "#         accuracy_reduction = baseline_accuracy - final_average_accuracy\n",
    "#         block_reductions.append(accuracy_reduction)\n",
    "#         total_accuracy_reduction += accuracy_reduction\n",
    "#         total_params_reduction += params_reduction[block_idx]\n",
    "#         total_macs_reduction += macs_reduction[block_idx] \n",
    "\n",
    "#     relative_contributions = []\n",
    "#     weighted_importance_scores = []\n",
    "\n",
    "#     # logging.info(\"\\n=== Relative Contribution of Each Block ===\")\n",
    "#     print(\"\\n=== Relative Contribution of Each Block ===\")\n",
    "#     for block_idx in range(blocks_number):\n",
    "#         relative_contribution_accuracy = (block_reductions[block_idx] / total_accuracy_reduction) * 100\n",
    "#         relative_contribution_params = (1 - (params_reduction[block_idx] / total_params_reduction)) * 100\n",
    "#         relative_contribution_macs = (1 - (macs_reduction[block_idx] / total_macs_reduction)) * 100\n",
    "\n",
    "#         weight_accuracy = 0.5\n",
    "#         weight_params = 0.3\n",
    "#         weight_macs = 0.2\n",
    "#         weighted_importance = (weight_accuracy * relative_contribution_accuracy) \n",
    "#         + (weight_params * relative_contribution_params) \n",
    "#         + (weight_macs * relative_contribution_macs)\n",
    "\n",
    "#         logging.info(\n",
    "#         f\"Block {block_idx}: Accuracy Decrease Contribution = {relative_contribution_accuracy:.2f}%, \"\n",
    "#         f\"Parameter Reduction = {100 - relative_contribution_params:.2f}%, \"\n",
    "#         f\"MACs Reduction = {100 - relative_contribution_macs:.2f}%, \"\n",
    "#         f\"Weighted Importance Score = {weighted_importance:.2f}\"\n",
    "#         )  \n",
    "#         print(f\"Block {block_idx}: Accuracy Decrease Contribution = {relative_contribution_accuracy:.2f}%, \")\n",
    "#         print(f\"Parameter Reduction = {100 - relative_contribution_params:.2f}%, \")\n",
    "#         print(f\"MACs Reduction = {100 - relative_contribution_macs:.2f}%, \")\n",
    "#         print(f\"Weighted Importance Score = {weighted_importance:.2f}\")\n",
    "\n",
    "#         relative_contributions.append(relative_contribution_accuracy)\n",
    "#         weighted_importance_scores.append(weighted_importance)\n",
    "\n",
    "#     return weighted_importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4c1deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(trained_model, prune_method, pruning_ratios, data_loader, device):\n",
    "   \n",
    "    # Make a copy of the trained model\n",
    "    model = copy.deepcopy(trained_model)\n",
    "    model.to(device)\n",
    "\n",
    "    model_blocks, ignored_layers = get_model_blocks(model)\n",
    "\n",
    "\n",
    "    pruning_info = {i: {\"block\": model_blocks[i], \"pruning_ratio\": ratio} \n",
    "                    for i, ratio in enumerate(pruning_ratios)}\n",
    "\n",
    "    if prune_method == 'Taylor':\n",
    "        imp = tp.importance.TaylorImportance() \n",
    "        \n",
    "        if isinstance(imp, (tp.importance.GroupTaylorImportance, tp.importance.GroupHessianImportance)):\n",
    "            model.zero_grad()\n",
    "            if isinstance(imp, tp.importance.GroupHessianImportance):\n",
    "                imp.zero_grad() # clear the accumulated gradients\n",
    "            print(\"Accumulating gradients for pruning...\")\n",
    "            for k, (imgs, lbls) in enumerate(tqdm(data_loader)):\n",
    "                if k>=taylor_batchs: break\n",
    "                imgs = imgs.to(device)\n",
    "                lbls = lbls.to(device)\n",
    "                output = model(imgs)\n",
    "                if isinstance(imp, tp.importance.GroupHessianImportance): # per-sample gradients for hessian\n",
    "                    loss = torch.nn.functional.cross_entropy(output, lbls, reduction='none')\n",
    "                    for l in loss:\n",
    "                        model.zero_grad()\n",
    "                        l.backward(retain_graph=True)\n",
    "                        imp.accumulate_grad(model) # accumulate gradients\n",
    "                elif isinstance(imp, tp.importance.GroupTaylorImportance): # batch gradients for first-order taylor\n",
    "                    loss = torch.nn.functional.cross_entropy(output, lbls)\n",
    "                    loss.backward()\n",
    "\n",
    "        original_macs, original_params = tp.utils.count_ops_and_params(model, imgs)\n",
    "\n",
    "        # Prune each block while ignoring other layers\n",
    "        for i, info in pruning_info.items():\n",
    "            pruning_ratio = info[\"pruning_ratio\"]\n",
    "            \n",
    "            # Add all blocks to the ignored layers except the block being pruned\n",
    "            ignored_layers_block = [pruning_info[j][\"block\"] for j in range(len(pruning_info)) if j != i]\n",
    "\n",
    "            # Combine fixed ignored layers (conv_stem, bn1, classifier) with the ignored blocks\n",
    "            combined_ignored_layers = ignored_layers + ignored_layers_block\n",
    "\n",
    "            # Apply pruning using the combined ignored layers\n",
    "            pruner_group = tp.pruner.MagnitudePruner( \n",
    "                model,\n",
    "                example_inputs=imgs,\n",
    "                importance=imp,\n",
    "                pruning_ratio=pruning_ratio,\n",
    "                ignored_layers=combined_ignored_layers,\n",
    "                iterative_steps=1,\n",
    "            )\n",
    "\n",
    "            # Step through pruning\n",
    "            pruner_group.step()\n",
    "\n",
    "    # Counting MACs and Params after pruning\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, imgs)\n",
    "\n",
    "    # logging.info(f\"MACs of the Pruned Model: {macs/ 1e9} G\")\n",
    "    print(f\"MACs of the Original Model: {original_macs/ 1e9} G  -->  MACs of the Pruned Model: {macs/ 1e9} G\")\n",
    "    # logging.info(f\"# Parameters of the Pruned Model: {nparams/ 1e3} K\")\n",
    "    print(f\"# Parameters of the Original Model: {original_params/ 1e3} K  --> # Parameters of the Pruned Model: {nparams/ 1e3} K\")\n",
    "\n",
    "    param_reduction = ((original_params - nparams) / original_params) * 100\n",
    "    macs_reduction = ((original_macs - macs) / original_macs) * 100\n",
    "\n",
    "    # Free up GPU memory\n",
    "    del imgs, lbls, output\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, math.ceil(param_reduction), math.ceil(macs_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43909cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pruning_ratios_intense(contributions, max_pruning_ratio=0.9, k=5):\n",
    "    # Normalize the contributions to get values between 0 and 1\n",
    "    total_contribution = sum(contributions)\n",
    "    normalized_contributions = [contribution / total_contribution for contribution in contributions]\n",
    "\n",
    "    # Apply exponential decay to magnify the effect for less important blocks\n",
    "    pruning_factors = [np.exp(-k * nc) for nc in normalized_contributions]\n",
    "\n",
    "    # Normalize the pruning factors so they stay within the max pruning ratio\n",
    "    max_factor = max(pruning_factors)\n",
    "    normalized_factors = [pf / max_factor for pf in pruning_factors]\n",
    "\n",
    "    # Scale by the maximum pruning ratio\n",
    "    pruning_ratios = [max_pruning_ratio * nf for nf in normalized_factors]\n",
    "\n",
    "    pruning_ratios = [round(num, 2) for num in pruning_ratios]\n",
    "\n",
    "    return pruning_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac8bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "relative_contribution = perplexity_analysis_with_contributions(model, 'Taylor', cal_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82813e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNext Blocks Contributions\n",
    "# relative_contribution = [29.51, 1.75, 4.72, 0.00, 0.81, 0.94, 0.81, 0.00, -0.40, 0.81, 0.54, 1.08, 0.81, -0.27, 0.81, -0.54, -0.13, 1.08, 0.27, 0.81, 0.67, 0.67, 0.40, \n",
    "#                          0.13, 0.81, 0.27, 0.67, 0.81, 0.67, 0.67, 0.00, -0.40, 0.00, 0.13, 0.54, 0.00, 0.27, 0.27, 0.00, 0.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244977e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.199222889438361,\n",
       " 0.30024726245143063,\n",
       " 0.9360649947015189,\n",
       " 9.46661956905687,\n",
       " 1.0243730130695867,\n",
       " 0.8300953726598375,\n",
       " 3.1967502649240553,\n",
       " 13.157894736842104,\n",
       " 0.24726245143058992,\n",
       " 0.33557046979865773,\n",
       " 0.37089367714588484,\n",
       " 0.4592016955139526,\n",
       " 0.4238784881667256,\n",
       " 0.31790886612504415,\n",
       " 0.688802543270929,\n",
       " 0.4238784881667256,\n",
       " 0.4238784881667256,\n",
       " 0.7594489579653833,\n",
       " 0.33557046979865773,\n",
       " 0.282585658777817,\n",
       " 0.26492405510420347,\n",
       " 0.4238784881667256,\n",
       " 0.08830801836806781,\n",
       " 0.12363122571529496,\n",
       " 0.44154009184033916,\n",
       " 0.17661603673613563,\n",
       " 0.31790886612504415,\n",
       " 0.40621688449311194,\n",
       " 0.3885552808194984,\n",
       " 0.7064641469445425,\n",
       " 3.07311903920876,\n",
       " 2.0840692334864004,\n",
       " 1.3246202755210172]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # ResNet Blocks Contributions\n",
    "\n",
    "# relative_contribution = [6.373626373626373,\n",
    "#  0.4578754578754579,\n",
    "#  0.9340659340659341,\n",
    "#  9.945054945054945,\n",
    "#  1.0622710622710623,\n",
    "#  0.5494505494505495,\n",
    "#  2.9487179487179485,\n",
    "#  13.754578754578755,\n",
    "#  0.1282051282051282,\n",
    "#  0.5311355311355311,\n",
    "#  0.29304029304029305,\n",
    "#  0.5677655677655677,\n",
    "#  0.2380952380952381,\n",
    "#  0.347985347985348,\n",
    "#  0.4578754578754579,\n",
    "#  0.4761904761904762,\n",
    "#  0.49450549450549447,\n",
    "#  0.6043956043956045,\n",
    "#  0.3663003663003663,\n",
    "#  0.4761904761904762,\n",
    "#  0.18315018315018314,\n",
    "#  0.29304029304029305,\n",
    "#  0.10989010989010989,\n",
    "#  0.29304029304029305,\n",
    "#  0.42124542124542125,\n",
    "#  0.2014652014652015,\n",
    "#  0.2014652014652015,\n",
    "#  0.402930402930403,\n",
    "#  0.3663003663003663,\n",
    "#  0.6227106227106227,\n",
    "#  3.296703296703297,\n",
    "#  1.52014652014652,\n",
    "#  1.0805860805860805]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4941f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5756327251324307,\n",
       " 3.5854424171081027,\n",
       " 3.5854424171081027,\n",
       " 3.5903472630959388,\n",
       " 2.687855601334118,\n",
       " 2.104178928781636,\n",
       " 3.5854424171081027,\n",
       " 3.4628212674122034,\n",
       " 0.5199136747106141,\n",
       " 1.0251128114577202,\n",
       " 3.5756327251324307,\n",
       " 2.530900529723367,\n",
       " 3.1832450461055526,\n",
       " 3.5854424171081027,\n",
       " 2.717284677261134,\n",
       " 3.0998626643123406,\n",
       " 3.5854424171081027]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResNet Blocks Contributions\n",
    "\n",
    "# relative_contribution = [3.5756327251324307,\n",
    "#  3.5854424171081027,\n",
    "#  3.5854424171081027,\n",
    "#  3.5903472630959388,\n",
    "#  2.687855601334118,\n",
    "#  2.104178928781636,\n",
    "#  3.5854424171081027,\n",
    "#  3.4628212674122034,\n",
    "#  0.5199136747106141,\n",
    "#  1.0251128114577202,\n",
    "#  3.5756327251324307,\n",
    "#  2.530900529723367,\n",
    "#  3.1832450461055526,\n",
    "#  3.5854424171081027,\n",
    "#  2.717284677261134,\n",
    "#  3.0998626643123406,\n",
    "#  3.5854424171081027]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4efc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_ratios = calculate_pruning_ratios_intense(relative_contribution, max_pruning_ratio=0.98, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a21c32e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating gradients for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:15<00:00, 64.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs of the Original Model: 15.360289896 G  -->  MACs of the Pruned Model: 2.679145091 G\n",
      "# Parameters of the Original Model: 88591.464 K  --> # Parameters of the Pruned Model: 13470.083 K\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pruned_model, param_reduction, macs_reduction = prune_model(model, 'Taylor', pruning_ratios, cal_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4df63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_accuracy,_ = validate_model(pruned_model, val_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2d2c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model, f'/home/ict317-3/Mohammad/Isomorphic-Pruning/our_pruned_models/ConvNext4.2/pruned_model_{param_reduction}%.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
